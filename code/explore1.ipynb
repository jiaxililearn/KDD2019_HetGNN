{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import data_generator\n",
    "from args import read_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(A_n=28646, P_n=21044, V_n=18, batch_s=300, checkpoint='', cuda=0, data_path='../data/custom_data_simple/', embed_d=26, in_f_d=128, lr=0.001, mini_batch_s=100, model_path='../model_save/', random_seed=10, save_model_freq=2, train_iter_n=50, train_test_label=0, walk_L=30, walk_n=10, window=5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = read_args()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading relation files a_a_list.txt\n",
      "Reading relation files a_b_list.txt\n",
      "Reading relation files a_c_list.txt\n",
      "\tProcessed 4999 lines\n",
      "Reading relation files a_d_list.txt\n",
      "\tProcessed 4999 lines\n",
      "Reading relation files a_e_list.txt\n",
      "\tProcessed 4999 lines\n",
      "Reading relation files a_f_list.txt\n",
      "Reading relation files a_g_list.txt\n",
      "\tProcessed 4999 lines\n",
      "Reading relation files a_h_list.txt\n",
      "Reading relation files b_a_list.txt\n",
      "Reading relation files b_b_list.txt\n",
      "Reading relation files b_c_list.txt\n",
      "\tProcessed 4999 lines\n",
      "\tProcessed 9999 lines\n",
      "\tProcessed 14999 lines\n",
      "\tProcessed 19999 lines\n",
      "\tProcessed 24999 lines\n",
      "Reading relation files b_d_list.txt\n",
      "\tProcessed 4999 lines\n",
      "\tProcessed 9999 lines\n",
      "\tProcessed 14999 lines\n",
      "Reading relation files b_e_list.txt\n",
      "\tProcessed 4999 lines\n",
      "\tProcessed 9999 lines\n",
      "Reading relation files b_h_list.txt\n",
      "Reading Node Edge Embedding file ../data/custom_data_simple/incoming_edge_embedding.csv\n",
      "Creating Neighbour Edge Embeddings\n"
     ]
    }
   ],
   "source": [
    "input_data = data_generator.input_data(args=args)\n",
    "\n",
    "# import json\n",
    "# with open('../data/custom_data/node_mapping.json') as fin:\n",
    "#     nodem = json.loads(fin.read())\n",
    "\n",
    "# tnodem = {v:k for k,v in nodem.items()} \n",
    "\n",
    "# tnodem['a2']\n",
    "\n",
    "# # nodem['2']\n",
    "\n",
    "# for i in range(len(input_data.a_neigh_list_train)):\n",
    "#     print(f'{i}: {input_data.a_neigh_list_train[i][6]}')\n",
    "\n",
    "# import torch\n",
    "\n",
    "# x = torch.randn(1, 3)\n",
    "# x\n",
    "\n",
    "# x.expand(32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_edge_embeddings = input_data.node_edge_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_edge_embeddings.shape\n",
    "# n_nodes = 5044482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_incoming_node_embedding = {}\n",
    "# incoming_node_embedding_size = node_edge_embeddings.shape[1] - 2\n",
    "\n",
    "# for row in node_edge_embeddings:\n",
    "#     gid = int(row[0])\n",
    "#     dst_id = int(row[1])\n",
    "    \n",
    "#     if gid not in graph_incoming_node_embedding.keys():\n",
    "#         graph_incoming_node_embedding[gid] = np.zeros((n_dst_node, incoming_node_embedding_size))\n",
    "\n",
    "#     graph_incoming_node_embedding[gid][dst_id] += row[2:]\n",
    "# graph_incoming_node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data.tmp['a_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_data.a_d_edge_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data.incoming_edge_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 3., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 3., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.a_a_edge_embed[[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 12, 26)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.a_a_edge_embed[[0,2]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 3., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 3., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.a_a_edge_embed[[0,2]].reshape(2, 12*26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 1, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.a_a_edge_embed[[0,2]].reshape(2, 1, 12*26).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [2., 2.],\n",
       "       [3., 3.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [2., 2.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.a_a_edge_embed[[0,2]].reshape(2, 12*26).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\n",
    "            input_data.a_a_edge_embed,\n",
    "            input_data.a_b_edge_embed,\n",
    "            input_data.a_c_edge_embed,\n",
    "            input_data.a_d_edge_embed,\n",
    "            input_data.a_e_edge_embed,\n",
    "            input_data.a_f_edge_embed,\n",
    "            input_data.a_g_edge_embed,\n",
    "            input_data.a_h_edge_embed,\n",
    "            input_data.b_a_edge_embed,\n",
    "            input_data.b_b_edge_embed,\n",
    "            input_data.b_c_edge_embed,\n",
    "            input_data.b_d_edge_embed,\n",
    "            input_data.b_e_edge_embed,\n",
    "            input_data.b_h_edge_embed\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, fl in enumerate(feature_list):\n",
    "            feature_list[i] = torch.from_numpy(np.array(feature_list[i])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 26])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train_id_list = input_data.train_graph_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_train_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tools.HetAgg(args, feature_list, graph_train_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HetAgg(\n",
       "  (fc_a_a_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_b_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_c_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_d_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_e_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_f_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_g_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_h_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_a_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_b_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_c_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_d_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_e_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_h_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_het_neigh_agg): Linear(in_features=364, out_features=26, bias=True)\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (bn1): BatchNorm1d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(364, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optim = optim.Adam(parameters, lr=args.lr, weight_decay=0)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.4094,  -6.5065, -12.1058, -13.2508,  -8.0205,  -5.5038,  -8.2451,\n",
       "         -12.8506, -15.2601,  -4.8469,  -8.9490,  -4.0014,  -7.8161,  -7.1718,\n",
       "          -8.7100,  -5.5961,  -2.0043, -11.7535, -12.0572,  -9.2990, -15.6931,\n",
       "          -8.5654,  -9.1663, -20.2892, -21.0299, -19.5732],\n",
       "        [-12.5751,  -6.8610,  -6.4315,  -8.2420,  -6.1343,  -7.4367,  -7.2967,\n",
       "          -8.4533,  -8.1905,  -4.3780,  -8.2866,  -4.5851,  -5.5488,  -5.6877,\n",
       "          -5.4663,  -6.1787,  -3.3881,  -9.3512,  -8.0886,  -7.5406, -13.0114,\n",
       "          -8.7566,  -8.1452, -15.0398, -15.9120, -13.6070]],\n",
       "       grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "model([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5960e+00, -4.9668e-02,  1.9191e+00,  1.6949e+00,  3.1416e-01,\n",
       "          2.2472e+00, -6.6078e-02,  1.2835e+00, -1.9116e-02, -2.2845e-04,\n",
       "         -3.4482e-03, -2.1231e-02, -3.9098e-02, -6.4082e-02,  9.6993e-01,\n",
       "          3.2110e+00,  6.6395e-01,  1.2734e+00,  4.2702e+00,  5.4182e+00,\n",
       "         -6.5543e-03, -4.1712e-02, -6.4573e-02, -2.1810e-03, -2.3831e-02,\n",
       "          7.5825e+00],\n",
       "        [ 5.1083e+00, -5.1790e-02,  2.0369e+00,  1.8667e+00, -4.9483e-04,\n",
       "          2.4112e+00, -6.6096e-02,  1.4960e+00, -2.5074e-02,  2.4765e-01,\n",
       "         -3.6351e-03, -2.3506e-02, -4.2307e-02, -6.9324e-02,  7.1641e-01,\n",
       "          3.3172e+00,  6.2097e-01,  1.1204e+00,  4.6376e+00,  5.7226e+00,\n",
       "         -5.1419e-03, -4.6511e-02, -6.7751e-02, -1.4799e-03, -2.6213e-02,\n",
       "          8.0953e+00]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.edge_content_agg([0,1], 'a_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svdd_batch_loss(embed_batch): #nu: {0.1, 0.01}\n",
    "    _batch_out = embed_batch\n",
    "    _batch_out_resahpe = _batch_out.view(_batch_out.size()[0] * _batch_out.size()[1], embed_d)\n",
    "    \n",
    "    hypersphere_center = torch.mean(_batch_out_resahpe, 0)\n",
    "    \n",
    "    dist = torch.square(_batch_out_resahpe - hypersphere_center)\n",
    "    \n",
    "    return torch.mean(torch.sum(dist, 1))\n",
    "\n",
    "# svdd_batch_loss(_out, batch_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 ...\n",
      "Batch Loss: 998769.1875\n",
      "Batch Loss: 19455156.0\n",
      "iteration 1 ...\n",
      "Batch Loss: 656990.375\n",
      "Batch Loss: 14246745.0\n",
      "iteration 2 ...\n",
      "Batch Loss: 419677.46875\n",
      "Batch Loss: 10036445.0\n",
      "iteration 3 ...\n",
      "Batch Loss: 257699.046875\n",
      "Batch Loss: 6800866.0\n",
      "iteration 4 ...\n",
      "Batch Loss: 151272.515625\n",
      "Batch Loss: 4402917.0\n",
      "iteration 5 ...\n",
      "Batch Loss: 83769.6953125\n",
      "Batch Loss: 2691663.0\n",
      "iteration 6 ...\n",
      "Batch Loss: 44503.828125\n",
      "Batch Loss: 1494286.125\n",
      "iteration 7 ...\n",
      "Batch Loss: 22803.3125\n",
      "Batch Loss: 735565.875\n",
      "iteration 8 ...\n",
      "Batch Loss: 10926.998046875\n",
      "Batch Loss: 327469.9375\n",
      "iteration 9 ...\n",
      "Batch Loss: 4741.4248046875\n",
      "Batch Loss: 147639.328125\n",
      "iteration 10 ...\n",
      "Batch Loss: 2171.076904296875\n",
      "Batch Loss: 74407.3125\n",
      "iteration 11 ...\n",
      "Batch Loss: 1261.9591064453125\n",
      "Batch Loss: 43297.19921875\n",
      "iteration 12 ...\n",
      "Batch Loss: 940.5889282226562\n",
      "Batch Loss: 26772.544921875\n",
      "iteration 13 ...\n",
      "Batch Loss: 806.6353759765625\n",
      "Batch Loss: 16400.189453125\n",
      "iteration 14 ...\n",
      "Batch Loss: 738.5416259765625\n",
      "Batch Loss: 9976.228515625\n",
      "iteration 15 ...\n",
      "Batch Loss: 700.9281005859375\n",
      "Batch Loss: 6447.265625\n",
      "iteration 16 ...\n",
      "Batch Loss: 677.9112548828125\n",
      "Batch Loss: 4751.314453125\n",
      "iteration 17 ...\n",
      "Batch Loss: 670.42919921875\n",
      "Batch Loss: 4031.75\n",
      "iteration 18 ...\n",
      "Batch Loss: 671.4129638671875\n",
      "Batch Loss: 3694.416748046875\n",
      "iteration 19 ...\n",
      "Batch Loss: 673.77099609375\n",
      "Batch Loss: 3593.847900390625\n",
      "iteration 20 ...\n",
      "Batch Loss: 675.9747314453125\n",
      "Batch Loss: 3609.462890625\n",
      "iteration 21 ...\n",
      "Batch Loss: 676.572265625\n",
      "Batch Loss: 3643.2919921875\n",
      "iteration 22 ...\n",
      "Batch Loss: 675.72021484375\n",
      "Batch Loss: 3671.4345703125\n",
      "iteration 23 ...\n",
      "Batch Loss: 673.9902954101562\n",
      "Batch Loss: 3693.689208984375\n",
      "iteration 24 ...\n",
      "Batch Loss: 671.8438720703125\n",
      "Batch Loss: 3709.328369140625\n",
      "iteration 25 ...\n",
      "Batch Loss: 669.6397094726562\n",
      "Batch Loss: 3716.677490234375\n",
      "iteration 26 ...\n",
      "Batch Loss: 667.6743774414062\n",
      "Batch Loss: 3715.890380859375\n",
      "iteration 27 ...\n",
      "Batch Loss: 666.185302734375\n",
      "Batch Loss: 3708.943359375\n",
      "iteration 28 ...\n",
      "Batch Loss: 665.2947998046875\n",
      "Batch Loss: 3698.694091796875\n",
      "iteration 29 ...\n",
      "Batch Loss: 664.9873046875\n",
      "Batch Loss: 3686.6474609375\n",
      "iteration 30 ...\n",
      "Batch Loss: 665.2075805664062\n",
      "Batch Loss: 3673.447509765625\n",
      "iteration 31 ...\n",
      "Batch Loss: 665.3685302734375\n",
      "Batch Loss: 3659.99365234375\n",
      "iteration 32 ...\n",
      "Batch Loss: 665.2614135742188\n",
      "Batch Loss: 3646.066650390625\n",
      "iteration 33 ...\n",
      "Batch Loss: 664.9334106445312\n",
      "Batch Loss: 3631.748779296875\n",
      "iteration 34 ...\n",
      "Batch Loss: 664.4263305664062\n",
      "Batch Loss: 3617.21240234375\n",
      "iteration 35 ...\n",
      "Batch Loss: 663.7718505859375\n",
      "Batch Loss: 3603.27587890625\n",
      "iteration 36 ...\n",
      "Batch Loss: 663.1018676757812\n",
      "Batch Loss: 3588.21044921875\n",
      "iteration 37 ...\n",
      "Batch Loss: 662.41064453125\n",
      "Batch Loss: 3573.84423828125\n",
      "iteration 38 ...\n",
      "Batch Loss: 661.6077880859375\n",
      "Batch Loss: 3560.382568359375\n",
      "iteration 39 ...\n",
      "Batch Loss: 660.8104248046875\n",
      "Batch Loss: 3545.31494140625\n",
      "iteration 40 ...\n",
      "Batch Loss: 660.0126342773438\n",
      "Batch Loss: 3531.109130859375\n",
      "iteration 41 ...\n",
      "Batch Loss: 659.1287231445312\n",
      "Batch Loss: 3516.8857421875\n",
      "iteration 42 ...\n",
      "Batch Loss: 658.1675415039062\n",
      "Batch Loss: 3502.501220703125\n",
      "iteration 43 ...\n",
      "Batch Loss: 657.1376342773438\n",
      "Batch Loss: 3487.982177734375\n",
      "iteration 44 ...\n",
      "Batch Loss: 656.0504760742188\n",
      "Batch Loss: 3473.37353515625\n",
      "iteration 45 ...\n",
      "Batch Loss: 654.916748046875\n",
      "Batch Loss: 3458.71728515625\n",
      "iteration 46 ...\n",
      "Batch Loss: 653.8171997070312\n",
      "Batch Loss: 3444.06494140625\n",
      "iteration 47 ...\n",
      "Batch Loss: 652.7649536132812\n",
      "Batch Loss: 3429.458740234375\n",
      "iteration 48 ...\n",
      "Batch Loss: 651.6957397460938\n",
      "Batch Loss: 3414.925537109375\n",
      "iteration 49 ...\n",
      "Batch Loss: 650.5946655273438\n",
      "Batch Loss: 3400.495849609375\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "batch_s = args.batch_s\n",
    "mini_batch_s = args.mini_batch_s\n",
    "embed_d = args.embed_d\n",
    "\n",
    "for iter_i in range(args.train_iter_n):\n",
    "    print('iteration ' + str(iter_i) + ' ...')\n",
    "    gid_list = np.array(range(600))\n",
    "    batch_list = gid_list.reshape(int(600 / batch_s), batch_s)\n",
    "\n",
    "    \n",
    "    for batch_n, k in enumerate(batch_list):\n",
    "        _out = torch.zeros(int(len(k) / mini_batch_s), mini_batch_s, embed_d)\n",
    "        mini_batch_list = k.reshape(int(len(k) / mini_batch_s), mini_batch_s)\n",
    "        for mini_n, mini_k in enumerate(mini_batch_list):\n",
    "            _out_temp = model(mini_k)\n",
    "            _out[mini_n] = _out_temp\n",
    "        \n",
    "        batch_loss = tools.svdd_batch_loss(_out)\n",
    "        print(f'Batch Loss: {batch_loss}')\n",
    "        optim.zero_grad()\n",
    "        batch_loss.backward(retain_graph=True)\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3400.495849609375"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 100, 26])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([657.5353, 863.0911, 615.0443,   7.6595,  11.6553,  17.7259,  -7.7687,\n",
       "        166.3391,  23.2053, 648.8605, 561.4694, 185.6070, 185.3958, 142.6107,\n",
       "         -6.4988,  -5.6927, 240.8286, 101.9214, 343.2222,  25.8779, 198.3611,\n",
       "         34.5414, 134.2992,  29.5433, 154.0036, 547.3102],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypersphere_center = torch.mean(_out[batch_n].view(len(mini_batch_list) * mini_batch_s, embed_d), 0)\n",
    "hypersphere_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 461.9786,  459.6783, -233.7465,  ...,  -33.1201, -159.7627,\n",
       "          367.8847],\n",
       "        [ 295.4781,  132.3753, -248.0656,  ...,  -32.7902, -155.9181,\n",
       "          104.9939],\n",
       "        [ 816.1484, 1347.2153,  857.7902,  ..., 1451.0356,  265.1805,\n",
       "          645.9609],\n",
       "        ...,\n",
       "        [ 169.4581, -119.2835,  154.2783,  ...,  447.9461, -155.6930,\n",
       "         -113.7345],\n",
       "        [-100.5992, -754.9912,  -57.0236,  ...,  -40.5429, -158.7739,\n",
       "           61.5334],\n",
       "        [-283.9245,  122.8851,  116.9337,  ...,  -30.9534,  300.8385,\n",
       "          557.7177]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out[batch_n].view(len(mini_batch_list) * mini_batch_s, embed_d) - hypersphere_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4155.9966, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.mean(torch.sum(torch.sqrt(torch.square(_out[batch_n].view(len(mini_batch_list) * mini_batch_s, embed_d) - hypersphere_center)), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1942913.5000, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.sum(torch.square(_out[batch_n].view(len(mini_batch_list) * mini_batch_s, embed_d) - hypersphere_center), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299.5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(range(600))\n",
    "np.random.shuffle(a)\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gid_list = np.array(range(600))\n",
    "benign_gid_list = all_gid_list[(all_gid_list < 300) | (all_gid_list > 399)]\n",
    "attack_gid_list = np.array(range(300, 400))\n",
    "\n",
    "# Train/Eval/Test = 0.6/0.2/0.2\n",
    "train_benign_gid_list = np.random.choice(benign_gid_list, 360, replace=False)\n",
    "left_benign_gid_list = benign_gid_list[np.in1d(\n",
    "    benign_gid_list, train_benign_gid_list, invert=True)]\n",
    "\n",
    "eval_benign_gid_list = np.random.choice(left_benign_gid_list, 70, replace=False)\n",
    "test_benign_gid_list = left_benign_gid_list[np.in1d(\n",
    "    left_benign_gid_list, eval_benign_gid_list, invert=True)]\n",
    "\n",
    "train_attack_gid_list = np.random.choice(attack_gid_list, 60, replace=False)\n",
    "left_attack_gid_list = attack_gid_list[np.in1d(\n",
    "    attack_gid_list, train_attack_gid_list, invert=True)]\n",
    "eval_attack_gid_list = np.random.choice(left_attack_gid_list, 20, replace=False)\n",
    "test_attack_gid_list = left_attack_gid_list[np.in1d(\n",
    "    left_attack_gid_list, eval_attack_gid_list, invert=True)]\n",
    "\n",
    "train_gid_list = np.concatenate([train_benign_gid_list, train_attack_gid_list], axis=0) \n",
    "eval_gid_list = np.concatenate([eval_benign_gid_list, eval_attack_gid_list], axis=0)\n",
    "test_gid_list = np.concatenate([test_benign_gid_list, test_attack_gid_list], axis=0)\n",
    "\n",
    "np.random.shuffle(train_gid_list)\n",
    "np.random.shuffle(eval_gid_list)\n",
    "np.random.shuffle(test_gid_list)\n",
    "np.random.shuffle(benign_gid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([395, 529, 552, 542, 462, 534,  40,  34, 179, 247, 480,  89,  45,\n",
       "       157, 114, 283, 550,  69, 402, 543, 476,  78,  26, 585, 199, 285,\n",
       "       267,  98, 341,  99, 230, 223, 404, 459,  42,  79, 464,  82, 165,\n",
       "       405, 184,  83, 259,   0, 557, 590, 394, 372, 272, 121, 486, 347,\n",
       "       517, 444, 554, 428, 384, 222, 271,  46, 280,  24, 148, 200, 207,\n",
       "        28, 562, 407, 409, 185,  54, 235,  13, 507, 385, 566, 158, 526,\n",
       "       286, 248, 256, 252, 155, 153, 110, 437, 133, 328, 494, 460, 545,\n",
       "       128, 573, 275, 596, 126,  65, 522, 589, 523, 423, 571, 583, 258,\n",
       "       310, 152, 196, 365, 598, 216, 375, 575, 503, 338, 490, 530, 445,\n",
       "       449, 333, 564, 304,  30, 364,  75, 474, 454, 380, 177, 466, 357,\n",
       "       171, 173, 297, 498, 332, 197, 346, 563, 371,  38,  80, 447, 298,\n",
       "       130, 142,  22, 509, 262, 349,  70, 591, 181, 147, 548, 439, 212,\n",
       "       456, 481,  81, 389, 241, 254, 546, 540, 138,  64, 426, 484,   8,\n",
       "       443, 170, 136,  14, 452, 166,  55, 475, 293, 581, 537, 593,  39,\n",
       "       226, 261, 279, 265,  37, 343,  47, 502, 576,  88, 264,  90, 113,\n",
       "       299, 206, 122, 103,  61, 535, 401, 353, 467, 538, 313,  92, 578,\n",
       "       109, 270, 129, 263, 547, 143, 376, 131, 183, 559, 154, 117,  29,\n",
       "       239, 145, 202, 144, 160, 422, 137, 499, 400,   7, 116, 276, 383,\n",
       "        68, 309,  25, 187, 374, 198, 210, 302, 242, 436, 479, 245,  31,\n",
       "       151, 415, 268,   2, 442, 378, 565, 392, 373, 553, 274, 296, 295,\n",
       "       427,  52, 528, 558, 211, 169, 350, 290, 465,  41, 180, 527, 100,\n",
       "       312, 524, 201, 555,  50, 132, 525, 337, 521, 186, 536, 382, 354,\n",
       "       441, 472, 320, 398, 506, 266,  48, 191, 424, 311, 217, 416, 366,\n",
       "       435, 470, 269, 463, 358, 345, 533, 225, 246, 478, 519,  17, 277,\n",
       "       250, 316,  32, 403, 205, 431, 330, 119, 489,   6,  20, 228, 300,\n",
       "        94,  10, 106, 244, 497, 339, 303, 584, 448,  36, 434, 257, 204,\n",
       "       570, 446, 240, 496, 123, 367, 305, 215, 387,  16,  84, 146, 572,\n",
       "       359,   9, 319, 231, 397,  93, 568, 323, 421, 511, 251,  63, 161,\n",
       "       120, 233, 582,  85, 111, 417, 488, 255, 561, 418, 471, 580, 482,\n",
       "       419, 175, 214, 515, 510, 485, 334, 493, 194, 308,  49, 176, 420,\n",
       "       278, 429,  21, 124, 495, 504, 118,  53, 294, 115,  11, 249, 208,\n",
       "       450, 455, 317, 412,  19, 178, 156, 500, 284, 579, 192, 386, 406,\n",
       "        77, 190,   3, 501])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([236, 520, 224, 599, 440,  74,   1, 551, 379, 492, 189, 105, 487,\n",
       "       544,  27, 411, 253, 306, 238, 307,  66, 348, 414, 410, 512, 438,\n",
       "       560, 425, 549, 229,  72, 597, 532, 483, 141, 291,  18, 458, 227,\n",
       "       469,  57, 390,  43, 352,   4, 326, 322, 288, 168, 336, 453, 577,\n",
       "       331, 167, 102, 569, 329,  33, 108,  12,  91, 135, 112, 361,  59,\n",
       "       393, 125, 574, 301,  44, 505, 508, 287, 209,  60, 159, 101, 362,\n",
       "       461, 172,  76, 360,  62, 388, 218,  71, 314, 213, 324, 344])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_gid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([335,  35,  86, 149, 150, 140, 104, 282, 107, 539, 237, 163, 588,\n",
       "       592, 234, 342, 232, 368, 219, 381, 315, 273, 413, 594, 327, 321,\n",
       "       399,  97,  87, 162, 243, 370, 325, 541, 432, 134, 457, 477, 391,\n",
       "       595,  51, 516, 513, 556, 164,  58,  15, 363,  67, 408, 531, 491,\n",
       "       340, 468, 473, 514, 281, 127, 260, 292, 174, 356, 430, 396, 351,\n",
       "        95, 369, 195,  56, 139,  96, 451, 377, 433, 193,  23, 182, 289,\n",
       "       220, 203, 518, 221,  73, 587,   5, 586, 318, 188, 355, 567])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((eval_gid_list >= 300) & (eval_gid_list < 400), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_X = model(eval_gid_list)\n",
    "true_y = np.where((eval_gid_list >= 300) & (eval_gid_list < 400), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(np.array(pred_X.tolist()), true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.07654476,  -3.67806578,  -3.97940493, ...,  -5.61856508,\n",
       "         -6.63739014,  -3.286201  ],\n",
       "       [-25.30461121, -17.8821907 , -19.46343231, ..., -28.46804428,\n",
       "        -23.69834137, -15.92532158],\n",
       "       [ -4.52741814,  -4.9840641 ,  -4.52318239, ...,  -5.44792175,\n",
       "         -7.24286461,  -6.37920046],\n",
       "       ...,\n",
       "       [-26.39011574, -21.42318153, -20.97297478, ..., -29.66191101,\n",
       "        -19.13190269, -20.55114174],\n",
       "       [ -4.7614584 ,  -4.15464735,  -2.90741062, ...,  -5.77147436,\n",
       "         -6.88473701,  -5.26079416],\n",
       "       [-29.96796799, -19.19325066, -22.16016579, ..., -27.75070953,\n",
       "        -31.17169762, -20.11503029]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
